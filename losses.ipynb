{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model_copy import CSTA\n",
    "# from train_task0 import CSTAConfig\n",
    "# model = CSTA(**vars(CSTAConfig))\n",
    "\n",
    "# import torch\n",
    "# model.load_state_dict(torch.load(\"Outputs/Models/Trial_51_run2/best_model.pth\"))\n",
    "\n",
    "# model.add_new_task_components(10+51)\n",
    "# model.freeze_all_but_last()\n",
    "\n",
    "# model.count_relations = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ucf_dataset_training_path_task0 = \"DATA/UCF101/tasks/task_0/train.csv\"\n",
    "ucf_dataset_test_path_task0 = \"DATA/UCF101/tasks/task_0/test.csv\"\n",
    "ucf_dataset_valid_path_task0 = \"DATA/UCF101/tasks/task_0/val.csv\"\n",
    "\n",
    "ucf_task1_train = \"DATA/UCF101/tasks/task_1/train.csv\"\n",
    "ucf_task1_valid = \"DATA/UCF101/tasks/task_1/val.csv\"\n",
    "ucf_task1_test = \"DATA/UCF101/tasks/task_1/test.csv\"\n",
    "\n",
    "train_ucf_task1 = pd.read_csv(ucf_task1_train)\n",
    "valid_ucf_task1 = pd.read_csv(ucf_task1_valid)\n",
    "ucf_train_task0 = pd.read_csv(ucf_dataset_training_path_task0)\n",
    "ucf_valid_task0 = pd.read_csv(ucf_dataset_valid_path_task0)\n",
    "\n",
    "task_0_labels = sorted(ucf_train_task0['label'].unique().tolist())\n",
    "task_1_labels = sorted(train_ucf_task1['label'].unique().tolist())\n",
    "\n",
    "all_labels = task_0_labels + task_1_labels\n",
    "\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "for index, label in enumerate(all_labels):\n",
    "    id2label[index] = label\n",
    "    label2id[label] = index\n",
    "from train_task0 import VideoDataset\n",
    "train_task1 = pd.read_csv(\"DATA/UCF101/tasks/task_1/train.csv\")\n",
    "train_dataset = VideoDataset(train_task1, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuserver3/workspace/kabir_works/csta/venv/lib/python3.9/site-packages/torchvision/io/video.py:197: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    }
   ],
   "source": [
    "data = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to('cuda')\n",
    "# data[\"input_frames\"] = data['input_frames'].to('cuda')\n",
    "# # data[\"label\"] = data['label'].to('cuda')\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     out = model(data['input_frames'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, pin_memory=False, persistent_workers=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# out = {}\n",
    "# for index, batch in enumerate(dataloader):\n",
    "#     batch['input_frames'] = batch['input_frames'].to('cuda')\n",
    "#     with torch.no_grad():\n",
    "#         batch_output = model(batch['input_frames'])\n",
    "#         batch_spatial_relations = batch_output.spatial_relations.detach().cpu()\n",
    "#         batch_temporal_relations = batch_output.temporal_relations.detach().cpu()\n",
    "#         out[index] = {'spatial_relations': batch_spatial_relations, 'temporal_relations': batch_temporal_relations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_spatial_relations = []\n",
    "# list_of_temporal_relations = []\n",
    "# for batch_number in out.keys():\n",
    "#     batch_spatial_relations = out[batch_number]['spatial_relations']\n",
    "#     batch_temporal_relations = out[batch_number]['temporal_relations']\n",
    "#     list_of_spatial_relations += batch_spatial_relations\n",
    "#     list_of_temporal_relations += batch_temporal_relations\n",
    "\n",
    "# list_of_spatial_relations = [tensor.tolist() for tensor in list_of_spatial_relations]\n",
    "# list_of_temporal_relations = [tensor.tolist() for tensor in list_of_temporal_relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal_relations_task1 = dict(enumerate(list_of_temporal_relations))\n",
    "# spatial_relations_task1 = dict(enumerate(list_of_spatial_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"DATA/UCF101/tasks/task_1/temporal_relations.json\", \"w\") as f:\n",
    "#     json.dump(temporal_relations_task1, f)\n",
    "    \n",
    "# with open(\"DATA/UCF101/tasks/task_1/spatial_relations.json\", \"w\") as f:\n",
    "#     json.dump(spatial_relations_task1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"DATA/UCF101/tasks/task_1/temporal_relations.json\", 'r') as f:\n",
    "#     temporal_relations = json.load(f)\n",
    "    \n",
    "# with open(\"DATA/UCF101/tasks/task_1/spatial_relations.json\", 'r') as f:\n",
    "#     spatial_relations = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporal_relations = {int(key): torch.tensor(value) for key, value in temporal_relations.items()}\n",
    "# spatial_relations = {int(key): torch.tensor(value) for key, value in spatial_relations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_copy import CSTA\n",
    "\n",
    "\n",
    "model = CSTA(num_classes=51, temporal_relations_path=\"DATA/UCF101/tasks/task_1/temporal_relations.json\", spatial_relations_path=\"DATA/UCF101/tasks/task_1/spatial_relations.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_new_task_components(51+10)\n",
    "model.freeze_all_but_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calculate_distil_loss, model.calculate_lt_ls_loss, model.count_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = torch.randn([10,8,3,224,224])\n",
    "label = torch.randint(0,10,[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(input, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = out.ls_loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "ls.mean(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
